{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import torch\n",
    "\n",
    "''' Replay Buffer\n",
    "'''\n",
    "N_CHANNEL, N_HIGH, N_WEIGHT = 50, 240, 240\n",
    "# ReplayMemory의 역할은 state, action, reward, next_state를 저장하는 것\n",
    "# state, action, reward, next_state를 저장해서 학습할 때 사용함.\n",
    "class ReplayMemory():\n",
    "    def __init__(self, memory_size):\n",
    "        # self.memory의 역할은 state, action, reward, next_state를 저장하는 것\n",
    "        self.memory_counter = 0\n",
    "        self.memory_size = memory_size\n",
    "        self.state_memory = torch.FloatTensor(self.memory_size, 1, N_CHANNEL, N_HIGH, N_WEIGHT)\n",
    "        self.action_memory = torch.LongTensor(self.memory_size)\n",
    "        self.reward_memory = torch.FloatTensor(self.memory_size)\n",
    "        self.state__memory = torch.FloatTensor(self.memory_size, 1, N_CHANNEL, N_HIGH, N_WEIGHT)\n",
    "\n",
    "    def store(self, s, a, r, s_):\n",
    "        # 여기를 보면 index에 맞게 \"순서대로\" 저장하는 것을 볼 수 있음.\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        \n",
    "        # print(s.shape, \"- (ReplayMemory.store)s.shape\")\n",
    "        self.state_memory[index] = s\n",
    "        self.action_memory[index] = torch.LongTensor([a.tolist()])\n",
    "        self.reward_memory[index] = torch.FloatTensor([r])\n",
    "        self.state__memory[index] = s_\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample(self, size):\n",
    "        sample_index = np.random.choice(self.memory_size, size)\n",
    "        state_sample = torch.FloatTensor(size,1,  N_CHANNEL, N_HIGH, N_WEIGHT)\n",
    "        action_sample = torch.LongTensor(size, 1)\n",
    "        reward_sample = torch.FloatTensor(size, 1)\n",
    "        state__sample = torch.FloatTensor(size, 1, N_CHANNEL, N_HIGH, N_WEIGHT)\n",
    "        for index in range(sample_index.size):\n",
    "            state_sample[index] = self.state_memory[sample_index[index]]\n",
    "            action_sample[index] = self.action_memory[sample_index[index]]\n",
    "            reward_sample[index] = self.reward_memory[sample_index[index]]\n",
    "            state__sample[index] = self.state__memory[sample_index[index]]\n",
    "        return state_sample, action_sample, reward_sample, state__sample\n",
    "\n",
    "\n",
    "''' Reshape state patches to full size input 50 x 256 x 256\n",
    "'''\n",
    "# crop_reshape 함수는 3D image를 (channel, 256, 256)으로 확장시키는 함수\n",
    "def crop_reshape(img, x,y,z,w,h,d):\n",
    "    img = np.array(img)\n",
    "    new_image = []\n",
    "    image_patch = img[x:x+h, y:y+w, z:z+d]\n",
    "    for i in range(image_patch.shape[2]):\n",
    "        new_image.append(cv2.resize(image_patch[:, :, i], (240, 240)))\n",
    "    new_image = np.array(new_image)\n",
    "    # print(new_image.shape, \"- (crop_reshape)new_image.shape\")\n",
    "    return_image = torch.tensor(new_image.reshape(1,1,new_image.shape[0], new_image.shape[1], new_image.shape[2])).float()\n",
    "    # print(return_image.shape, \"- (crop_reshape)return_image.shape\")\n",
    "    return return_image\n",
    "\n",
    "'''Compute Accuracy for Training \n",
    "'''\n",
    "\n",
    "# binary_acc함수는 y_pred와 y_test를 받아서 정확도를 계산하는 함수\n",
    "def binary_acc(y_pred, y_test):\n",
    "    #y_pred[y_pred >= 0.5] = 1 \n",
    "    #y_pred[y_pred < 0.5] = 0\n",
    "    _, y_pred = y_pred.max(1)\n",
    "    correct_results_sum = (y_pred == y_test).sum().float()\n",
    "    acc = correct_results_sum#/y_test.shape[0]\n",
    "    acc = torch.round(acc)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Dmodels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./src/DQN_based_Localization_figure.png\" alt=\"Description of the image\" width=\"600\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d,MaxUnpool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from skimage import util\n",
    "from torchvision import transforms, models\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "\n",
    "# DQNSolver함수는 3D image를 받아서 7개의 action을 출력하는 DQN 모델을 생성하는 함수\n",
    "# DQN이란 Deep Q Network의 약자로 Q-learning을 딥러닝으로 구현한 것\n",
    "# 현재는 50,256, 256의 3D image를 받아서 7개의 action을 출력하는 모델을 생성함.\n",
    "# 근데 우리의 Data는 50, 240, 240의 shape이기 때문에 이 부분을 수정해야함.\n",
    "class DQNSolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Net with 3 conv3D layers and two linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(input_shape[0], 32, kernel_size=(5,5,5), stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(5,5,5), stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        # conv_out_size의 역할은 conv layer를 거치고 나온 output의 size를 계산하는 것\n",
    "        # _get_conv_out에 어떤 shape이 들어가냐면, \n",
    "        # DQNSolver의 예상 input_shape의 크기가 (1,50,256,256)라면\n",
    "        # DQNSolver의 예상 input_shape은 (1, 50, 240, 240)라면, output의 shape은 (1, 64, 3, 3, 3)이 됨.\n",
    "        # 즉, 64 * 3 * 3 * 3 = 1728이 됨.\n",
    "        # 이걸 self.fc layer를 이용해 7개로 줄여 action을 결정하게 됨.\n",
    "        # 여기서 7개의 액션은 각각의 방향으로 움직이는 것을 의미함.\n",
    "        # 0 - up, 1- down, 2 - right, 3- left, 4- top, 5 - bottom, 6 - terminate\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        # print(conv_out.size(), \"- DQNSolver.conv_out size\")\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Net with 3 conv3D layers and two linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(input_shape[0], 32, kernel_size=(5,5,5), stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(5,5,5), stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        # print(conv_out.size(), \"- Classifier.conv_out size\")\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "# concat_\n",
    "class concat_classifier(nn.Module):\n",
    "    def __init__(self, input_shape1,input_shape2, classes):\n",
    "        super(concat_classifier, self).__init__()\n",
    "\n",
    "        input_shape = input_shape1[1] + input_shape2[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, classes),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self,x1, x2):\n",
    "        cat_out = torch.cat((x1,x2), dim = 1)\n",
    "        output = self.fc(cat_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "class combine_model(nn.Module):\n",
    "    def __init__(self, model1, model2, model3):\n",
    "        super(combine_model, self).__init__()\n",
    "\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        input1 = x1\n",
    "        input2 = x2\n",
    "\n",
    "        out1 = self.model1(input1)\n",
    "        out2 = self.model2(input2)\n",
    "        pred = self.model3(out1, out2)\n",
    "\n",
    "        return pred\n",
    "\n",
    "# 우리 데이터의 input shape이 어떻게 되는지 확인해야함\n",
    "policy_model = DQNSolver((1,50,240,240), 7)\n",
    "target_model = DQNSolver((1,50,240,240), 7)\n",
    "\n",
    "# hippocampus는 3D shape이 50, 256, 256인 이미지를 받아서 512개의 feature로 변환\n",
    "# hippocampus란 해마를 의미함.\n",
    "# classifier_hippocampus = Classifier((1,50,256,256), 512)\n",
    "\n",
    "# global은 3D shape이 145, 256, 256인 이미지를 받아서 1024개의 feature로 변환\n",
    "# TODO: 둘 차이가 많이 나는데 이게 맞는건지 확인\n",
    "# classifier_global = Classifier((1,145,256,256), 1024)\n",
    "# final_model = concat_classifier((1,512), (1, 1024), 2)\n",
    "\n",
    "# final_model이 2개의 feature를 받아서 2개의 class로 분류하는 모델\n",
    "# 현재의 프로젝트 맥락에서 각각은 AD, CN을 의미함.\n",
    "# 여기서 AD는 Alzheimer's Disease, CN은 Control을 의미함.\n",
    "# class_model = combine_model(classifier_hippocampus, classifier_global, final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d,MaxUnpool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from skimage import util\n",
    "from torchvision import transforms, models\n",
    "from torch import optim\n",
    "\n",
    "from medpy.io import load\n",
    "import cv2\n",
    "\n",
    "''' DQN Agent\n",
    "'''\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, policy_model, target_model, replay_memory):\n",
    "            self.batch_size = 100\n",
    "            self.gamma = 0.99\n",
    "            self.eps = 1\n",
    "            self.target_update = 0\n",
    "            screen_height, screen_width, screen_depth = 240, 240, 150\n",
    "            self.n_actions = 7\n",
    "            self.policy_model = policy_model\n",
    "            self.target_model = target_model\n",
    "            self.target_model.eval()\n",
    "            self.optimizer = optim.Adam(self.policy_model.parameters(), lr = 0.00009)\n",
    "            self.loss_func = torch.nn.SmoothL1Loss()#nn.MSELoss()\n",
    "            self.memory = replay_memory\n",
    "\n",
    "    ''' Compute reward with L2 distance between centers of state and gd\n",
    "    '''\n",
    "    def compute_reward(self, actual_state, prev_state, ground_truth, threshold):\n",
    "            x,y,z,w,h,d = actual_state\n",
    "            x_p,y_p,z_p,w_p,h_p,d_p = prev_state\n",
    "            x_gd, y_gd, z_gd, w_gd, h_gd, d_gd = ground_truth\n",
    "\n",
    "            center = np.array([(x+w)/2,(y+h)/2, (z+d)/2])\n",
    "            center_gd = np.array([(x_gd + w_gd)/2,(y_gd + h_gd)/2, (z_gd + d_gd)/2])\n",
    "            center_p = np.array([(x_p + w_p)/2,(y_p + h_p )/2, (z_p + d_p)/2])\n",
    "            dist_l2 = np.linalg.norm(center - center_gd)\n",
    "            dist_l2_p = np.linalg.norm(center_p - center_gd)\n",
    "\n",
    "            if dist_l2 < threshold:\n",
    "                game = \"END\"\n",
    "                reward = 100\n",
    "                return game, reward\n",
    "            else:\n",
    "                game = \"continue\"\n",
    "                reward = min(1, -dist_l2) #dist_l2 - dist_l2_p #\n",
    "                return game, reward\n",
    "\n",
    "    ''' 0 - up, 1- down, 2 - right, 3- left, 4- top, 5 - bottom, 6 - terminate\n",
    "    '''\n",
    "    def next_state(self, prev_state, actn, step):\n",
    "        # TODO: 일단 우리 data는 240x240x155이므로 그에 맞게 코드를 수정하던가 우리의 데이터를 interpolation해야할 듯 함.\n",
    "        # 근데 이미 interpolation하는 코드가 있어서, 그걸 사용하면 될 듯함.\n",
    "        x,y,z,w,h,d = prev_state\n",
    "        # max_x:가 왜 255인지 모르겠음\n",
    "        max_x = 240\n",
    "        max_y = 240\n",
    "        max_z = 150\n",
    "        min_x = 0\n",
    "        min_y = 0\n",
    "        min_z = 0\n",
    "\n",
    "        if actn == 0:\n",
    "            if x + w + step >= max_x:\n",
    "                x = 100\n",
    "            else:\n",
    "                x = x + step\n",
    "        elif actn == 1:\n",
    "            if x - step <= 0:\n",
    "                x = 100\n",
    "            else:\n",
    "                x = x - step\n",
    "        elif actn == 2:\n",
    "            if y + h + step >= max_y:\n",
    "                y = 100\n",
    "            else:\n",
    "                y = y + step\n",
    "        elif actn == 3:\n",
    "            if y - step <= 0:\n",
    "                y = 100\n",
    "            else:\n",
    "                y = y - step\n",
    "        elif actn == 4:\n",
    "            if z + d + step >= max_z :\n",
    "                z = 50\n",
    "            else:\n",
    "                z = z + step\n",
    "        elif actn == 5:\n",
    "            if z - step <= 0:\n",
    "                z = 50\n",
    "            else:\n",
    "                z = z - step\n",
    "        return [int(x),int(y),int(z),int(w),int(h),int(d)]\n",
    "    # def next_state(self, prev_state, actn, step):\n",
    "    #     # Constants for boundaries\n",
    "    #     max_bounds = {'x': 240, 'y': 240, 'z': 150}\n",
    "    #     min_bounds = {'x': 0, 'y': 0, 'z': 0}\n",
    "        \n",
    "    #     x, y, z, w, h, d = prev_state\n",
    "\n",
    "    #     # Action mapping\n",
    "    #     actions = {\n",
    "    #         0: ('x', step, max_bounds['x'], 100),\n",
    "    #         1: ('x', -step, min_bounds['x'], 100),\n",
    "    #         2: ('y', step, max_bounds['y'], 100),\n",
    "    #         3: ('y', -step, min_bounds['y'], 100),\n",
    "    #         4: ('z', step, max_bounds['z'], 50),\n",
    "    #         5: ('z', -step, min_bounds['z'], 50)\n",
    "    #     }\n",
    "        \n",
    "    #     # Get the corresponding parameters for the action\n",
    "    #     axis, movement, bound, reset_value = actions[int(actn.item())]\n",
    "        \n",
    "    #     # Apply the movement and boundary checks\n",
    "    #     if axis == 'x':\n",
    "    #         x = self.update_coordinate(x, movement, bound, reset_value)\n",
    "    #     elif axis == 'y':\n",
    "    #         y = self.update_coordinate(y, movement, bound, reset_value)\n",
    "    #     elif axis == 'z':\n",
    "    #         z = self.update_coordinate(z, movement, bound, reset_value)\n",
    "        \n",
    "    #     return [int(x),int(y),int(z),int(w),int(h),int(d)]\n",
    "\n",
    "    # def update_coordinate(self, coord, movement, bound, reset_value):\n",
    "    #     new_coord = coord + movement\n",
    "    #     if movement > 0:  # Moving forward\n",
    "    #         if new_coord >= bound:\n",
    "    #             return reset_value\n",
    "    #     else:  # Moving backward\n",
    "    #         if new_coord <= bound:\n",
    "    #             return reset_value\n",
    "    #     return new_coord\n",
    "\n",
    "\n",
    "    ''' Start with high eps then reduce eps, if game = END then action should be 6\n",
    "    '''\n",
    "    # epsilon-greedy 방법을 이용해서 action을 선택하는 함수\n",
    "    def select_action(self, state, game, eps ):\n",
    "            actn = 6\n",
    "            if game == \"continue\":\n",
    "                sample = random.random()\n",
    "                if sample < eps:\n",
    "                    actn = np.asarray(random.randrange(6))\n",
    "\n",
    "                else:\n",
    "                    # state의 예상 shape은 (1, 50, 240, 240)이 됨.\n",
    "                    # print(state.shape, \"- state shape\")\n",
    "                    out = self.policy_model(state)\n",
    "                    _, actn = torch.max(out.data, 1)\n",
    "\n",
    "                actn = np.array(actn)\n",
    "                return actn\n",
    "            else:\n",
    "                actn = np.array(actn)\n",
    "                return actn\n",
    "\n",
    "    def select_action_test(self, state):\n",
    "        out = self.target_model(state)\n",
    "        _, actn = torch.max(out.data,1)\n",
    "        return actn\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "            self.memory.store(s, a, r, s_)\n",
    "\n",
    "    ''' Optimize model with replay memory\n",
    "    '''\n",
    "\n",
    "    def optimize(self, counter):\n",
    "\n",
    "            if self.target_update > counter:\n",
    "                self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "                self.target_update = 0\n",
    "                #print(\"Updating Target Net\")\n",
    "\n",
    "            self.target_update += 1\n",
    "\n",
    "\n",
    "            s_s, a_s, r_s, s__s = self.memory.sample(self.batch_size)\n",
    "\n",
    "            q_eval = self.policy_model(s_s).gather(1, a_s)\n",
    "            q_next = self.target_model(s__s).detach()\n",
    "            q_target = r_s + self.gamma * q_next.max(1)[0].view(self.batch_size, 1)\n",
    "            loss = self.loss_func(q_eval, q_target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            return loss\n",
    "\n",
    "my_dqn = DQNAgent(policy_model, target_model, ReplayMemory(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../archive_BraTS2022'\n",
    "TRAIN_PATH = f'{DATA_PATH}/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "TEST_PATH = f'{DATA_PATH}/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "DATA_TYPES = ['flair', 't1', 't1ce', 't2', 'seg']\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nibabel as nib\n",
    "# from glob import glob\n",
    "# import re\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# def load_brats_data(path):\n",
    "#     print('load_brats_data...')\n",
    "#     files_path = []\n",
    "#     mask_paths = []\n",
    "#     unique_id = []\n",
    "#     for file_path in glob(f'{path}/*/*.nii'):\n",
    "#         # t1 파일만 처리\n",
    "#         if re.search(r\"_t1(?=\\.)\", file_path):\n",
    "#             files_path.append(file_path)\n",
    "#         # seg 파일만 처리\n",
    "#         if \"seg\" in file_path:  # segmentation mask file\n",
    "#             mask_paths.append(file_path)\n",
    "#         unique_id.append(str(int(file_path.split('_')[-2])))\n",
    "#     print(len(files_path), len(mask_paths), len(unique_id), \" - len(files_path), len(mask_paths), len(unique_id)\")\n",
    "#     print(files_path[:2], mask_paths[:2], unique_id[:2], \" - files_path, mask_paths, unique_id\")\n",
    "#     print(\"load_brats_data done.\")\n",
    "#     return sorted(files_path), sorted(mask_paths), sorted(list(set(unique_id)), key=lambda x: int(x))\n",
    "\n",
    "# var1 = 0\n",
    "# var2 = 150\n",
    "# def compute_brats_masks(mask_paths, var1, var2):\n",
    "#     print('compute_brats_masks...')\n",
    "#     all_masks = []\n",
    "#     for mask_path in tqdm(mask_paths):\n",
    "#         mask = nib.load(mask_path).get_fdata()  # 마스크 데이터 로드\n",
    "#         dummy_var = []\n",
    "#         for j in range(min(var2, mask.shape[-1])):\n",
    "#             if j <= var2:  # 주어진 범위 내의 슬라이스만 처리\n",
    "#                 msk = mask[:, :, j] / mask.max() * 255  # 슬라이스 정규화 및 스케일링(0 ~ 1)\n",
    "#                 # msk = cv2.resize(msk, (256, 256))  # 이미지 크기 조정\n",
    "#                 _, bw_img = cv2.threshold(msk.astype(np.uint8), 10, 255, cv2.THRESH_OTSU)  # 이진화\n",
    "#                 contours, _ = cv2.findContours(bw_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)  # 컨투어 찾기\n",
    "                \n",
    "#                 # 가장 큰 컨투어 찾기\n",
    "#                 max_area = 0\n",
    "#                 max_rect = (0, 0, 0, 0)\n",
    "#                 for contour in contours:\n",
    "#                     rect = cv2.boundingRect(contour)\n",
    "#                     area = rect[2] * rect[3]  # rectangle의 넓이 (width * height)\n",
    "#                     if area > max_area:\n",
    "#                         max_area = area\n",
    "#                         max_rect = rect\n",
    "                \n",
    "#                 # 가장 큰 컨투어의 사각형 정보 저장\n",
    "#                 if max_area > 0:\n",
    "#                     dummy_var.append([j, max_rect])\n",
    "#                 else:\n",
    "#                     dummy_rect = [0, 0, 0, 0]  # 컨투어가 없을 경우\n",
    "#                     dummy_var.append([j, dummy_rect])\n",
    "#         all_masks.append(dummy_var)\n",
    "    \n",
    "#     # np_all_masks는 단순히 shape을 위해 만든 것\n",
    "#     np_all_masks = np.array(all_masks, dtype=object)\n",
    "#     print(np_all_masks.shape, \" - np_all_masks.shape\")\n",
    "#     print(\"compute_brats_masks done.\")\n",
    "#     return all_masks\n",
    "\n",
    "\n",
    "# def brats_markers(slices_mask):\n",
    "#     print('brats_markers...')\n",
    "#     marks = []\n",
    "#     for i in tqdm(range(len(slices_mask))):\n",
    "#         x_min = []\n",
    "#         y_min = []\n",
    "#         w_max = []\n",
    "#         h_max = []\n",
    "#         indx_diff = []\n",
    "#         for j in range(len(slices_mask[i])):\n",
    "#             x_min.append(slices_mask[i][j][1][0])\n",
    "#             y_min.append(slices_mask[i][j][1][1])\n",
    "#             w_max.append(slices_mask[i][j][1][2])\n",
    "#             h_max.append(slices_mask[i][j][1][3])\n",
    "\n",
    "#             if slices_mask[i][j][1][0] != 0:\n",
    "#                 indx_diff.append(slices_mask[i][j][0])\n",
    "#         x_min = np.array(x_min)\n",
    "#         y_min = np.array(y_min)\n",
    "#         x_mn = np.min(x_min[np.nonzero(x_min)])\n",
    "#         y_mn = np.min(y_min[np.nonzero(y_min)])\n",
    "#         w_mx = np.array(w_max).max()\n",
    "#         h_mx = np.array(h_max).max()\n",
    "#         dim = [x_mn, y_mn, w_mx, h_mx]\n",
    "#         d_mx = indx_diff[-1] - indx_diff[0]\n",
    "#         z_mn = indx_diff[0]\n",
    "#         marks.append([x_mn, y_mn, z_mn, w_mx, h_mx, d_mx])\n",
    "#     print(marks[:2], \" - marks[:2]\")\n",
    "#     print(\"brats_markers done.\")\n",
    "#     return marks\n",
    "\n",
    "# all_files_id, mask_paths, unique_id = load_brats_data(TRAIN_PATH)\n",
    "# sliced_masks = compute_brats_masks(mask_paths, var1, var2)\n",
    "# sliced_markers = brats_markers(sliced_masks)\n",
    "# print(sliced_masks[:2], sliced_markers[:2], \" - sliced_masks, sliced_markers\")\n",
    "# # # Create labels (example: 1 for tumor presence, 0 for no tumor)\n",
    "# # labels = np.ones((len(sliced_markers), 1))  # Assuming all masks are tumors\n",
    "\n",
    "# # label_data = np.hstack([sliced_markers, labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Prep - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import fnmatch\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# from matplotlib import pyplot as plt\n",
    "# import PIL\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.decomposition import PCA\n",
    "# #from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# from medpy.io import load\n",
    "# import cv2\n",
    "\n",
    "# from collections import namedtuple, deque\n",
    "# import math\n",
    "# from sklearn.utils import shuffle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# ''' Read files for AD and CN\n",
    "# '''\n",
    "\n",
    "# # def mask_load(path):\n",
    "#     # files_id = []\n",
    "#     # mask = []\n",
    "#     # unique_id = []\n",
    "#     # for root, folder, file in os.walk(os.path.abspath(path)):\n",
    "#     #     for masks in fnmatch.filter(file,\"*.nii\"):\n",
    "#     #         y = os.path.join(root, masks).rsplit(\"\\\\\")[6]\n",
    "#     #         uid = os.path.join(root, masks).rsplit(\"\\\\\")[8]\n",
    "#     #         mask.append(os.path.join(root, masks))\n",
    "#     #         files_id.append(y)\n",
    "#     #         unique_id.append(uid)\n",
    "#     # return files_id, mask, unique_id\n",
    "\n",
    "# # path_ad = \"Path to AD\\\\ADNI\"\n",
    "# # all_files_ad_id, mask_ad, ad_unique_id = mask_load(path_ad)\n",
    "\n",
    "# # path_cn = \"Path to \\\\CN\\\\ADNI\"\n",
    "# # all_files_cn_id, mask_cn, cn_unique_id = mask_load(path_cn)\n",
    "\n",
    "\n",
    "# brats_file_paths, brats_mask_paths, brats_unique_id = load_brats_data(TRAIN_PATH)\n",
    "# #MP-RAGE_, SAG_MP-RAGE, MP-RAGE-, MPRAGE, MPRAGE_, MP-RAGE\n",
    "\n",
    "# # def data_load(path, file_id, unique_id):\n",
    "# #   file_path = []\n",
    "# #   f_path = []\n",
    "# #   for root, folder, file in os.walk(os.path.abspath(path)):\n",
    "# #     f_path = []\n",
    "# #     for filename in fnmatch.filter(file, \"*.dcm\" ):\n",
    "# #       id1 = os.path.join(root, filename).rsplit(\"\\\\\")[6]\n",
    "# #       if id1 in file_id:\n",
    "# #           id2 = os.path.join(root, filename).rsplit(\"\\\\\")[8]\n",
    "# #           if id2 in unique_id:\n",
    "# #               x = os.path.join(root, filename).rsplit(\"\\\\\")[7]\n",
    "# #               if (x == \"MP-RAGE_\" or x == \"SAG_MP-RAGE\" or x == \"MP-RAGE-\" or\n",
    "# #                    x == \"MPRAGE\" or x == \"MPRAGE_\" or x == \"MP-RAGE\") :\n",
    "# #                        f_path.append(os.path.join(root, filename))\n",
    "\n",
    "# #     if len(f_path) != 0:\n",
    "# #         file_path.append(f_path)\n",
    "# #   return file_path\n",
    "\n",
    "# # all_files_ad = data_load(path_ad, all_files_ad_id, ad_unique_id )\n",
    "\n",
    "# # def data_load_cn(path, file_id, unique_id):\n",
    "# #   file_path = []\n",
    "# #   f_path = []\n",
    "# #   for root, folder, file in os.walk(os.path.abspath(path)):\n",
    "# #     f_path = []\n",
    "# #     for filename in fnmatch.filter(file, \"*.dcm\" ):\n",
    "# #       id1 = os.path.join(root, filename).rsplit(\"\\\\\")[6]\n",
    "# #       if id1 in file_id:\n",
    "# #           id2 = os.path.join(root, filename).rsplit(\"\\\\\")[8]\n",
    "# #           if id2 in unique_id:\n",
    "# #               x = os.path.join(root, filename).rsplit(\"\\\\\")[7]\n",
    "# #               if ( x == \"MP-RAGE_\" or x == \"MP-RAGE-\" or x == \"MPRAGE\" or\n",
    "# #                    x == \"SAG_MP-RAGE_\" or x == \"MPRAGE_\" or x == \"MP_RAGE\" or\n",
    "# #                    x == \"MP-RAGE\" or x == \"MP-RAGE_\" or x == \"MP-RAGE__SERIES_2_\" or\n",
    "# #                    x == \"SAG_MP-RAGE\") :\n",
    "# #                        f_path.append(os.path.join(root, filename))\n",
    "\n",
    "# #     if len(f_path) != 0:\n",
    "# #         file_path.append(f_path)\n",
    "# #   return file_path\n",
    "\n",
    "# # all_files_cn = data_load_cn(path_cn, all_files_cn_id, cn_unique_id)\n",
    "\n",
    "# ''' Choose slices 25 - 125\n",
    "# '''\n",
    "# # 원래는 slice마다 image가 있어서, 이걸 전부 붙이는 과정이 있었는 듯 함.\n",
    "# var1 = 15\n",
    "# var2 = 159\n",
    "# # def slice_select(var1, var2, file_path):\n",
    "# #     new_filepath = []\n",
    "# #     for i in range(len(file_path)):\n",
    "# #         dummy_var = []\n",
    "# #         for j in range(len(file_path[i])):\n",
    "# #             if j>=var1 and j<= var2:\n",
    "# #                 dummy_var.append(file_path[i][j])\n",
    "# #         new_filepath.append(dummy_var)\n",
    "# #     return new_filepath\n",
    "\n",
    "# # sliced_all_files_cn = slice_select(var1, var2, all_files_cn)\n",
    "# # sliced_all_files_ad = slice_select(var1, var2, all_files_ad)\n",
    "\n",
    "# ''' mask selection\n",
    "# '''\n",
    "\n",
    "# def compute_all_masks(var1, var2, file):\n",
    "#     all_masks = []\n",
    "#     for i in range(len(file)):\n",
    "#         mask1,_ = load(file[i])\n",
    "#         dummy_var = []\n",
    "#         for j in range(mask1.shape[0]):\n",
    "#             if j >= var1 and j <= var2:\n",
    "#                 msk = 255*mask1[j]/mask1[j].max()\n",
    "#                 msk = cv2.resize(msk, (256,256))\n",
    "#                 _, bw_img = cv2.threshold(msk.astype(np.uint8), 10, 255, cv2.THRESH_OTSU)\n",
    "#                 # _, bw_img = cv2.threshold(masks[indx_mx], 127, 255, cv2.THRESH_BINARY)\n",
    "#                 contours,_ = cv2.findContours(bw_img,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#                 if len(contours) != 0:\n",
    "#                     # TODO: contours에서 가장 큰 것을 rect에 저장되도록 코드 수정.\n",
    "#                     # 그래서 2D image마다 contours를 찾아서 저장하고 있음, 이를 수정해야함.\n",
    "#                     rect =  cv2.boundingRect(contours[0])\n",
    "#                     dummy_var.append([j, rect])\n",
    "#                 else:\n",
    "#                     dummy_rect = [0,0,0,0]\n",
    "#                     dummy_var.append([j, dummy_rect])\n",
    "\n",
    "#         all_masks.append(dummy_var)\n",
    "#     return all_masks\n",
    "\n",
    "# def compute_all_brats_masks(var1, var2, file):\n",
    "#     all_masks = []\n",
    "#     for i in range(len(file)):\n",
    "#         mask1,_ = load(file[i])\n",
    "#         dummy_var = []\n",
    "#         for j in range(mask1.shape[0]):\n",
    "#             if j >= var1 and j <= var2:\n",
    "#                 msk = 255*mask1[j]/mask1[j].max()\n",
    "#                 msk = cv2.resize(msk, (256,256))\n",
    "#                 _, bw_img = cv2.threshold(msk.astype(np.uint8), 10, 255, cv2.THRESH_OTSU)\n",
    "#                 # _, bw_img = cv2.threshold(masks[indx_mx], 127, 255, cv2.THRESH_BINARY)\n",
    "#                 contours,_ = cv2.findContours(bw_img,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#                 if len(contours) != 0:\n",
    "#                     # TODO: contours에서 가장 큰 것을 rect에 저장되도록 코드 수정.\n",
    "#                     # 그래서 2D image마다 contours를 찾아서 저장하고 있음, 이를 수정해야함.\n",
    "#                     rect =  cv2.boundingRect(contours[0])\n",
    "#                     dummy_var.append([j, rect])\n",
    "#                 else:\n",
    "#                     dummy_rect = [0,0,0,0]\n",
    "#                     dummy_var.append([j, dummy_rect])\n",
    "\n",
    "#         all_masks.append(dummy_var)\n",
    "#     return all_masks\n",
    "\n",
    "\n",
    "# # mask_cn의 예상 입력은 file_paths이다. \n",
    "# # sliced_masks_cn = compute_all_masks(var1, var2, mask_cn)\n",
    "# # sliced_masks_ad = compute_all_masks(var1, var2, mask_ad)\n",
    "\n",
    "\n",
    "# brats_sliced_masks = compute_all_masks(var1, var2, brats_mask_paths)\n",
    "\n",
    "# ''' Select mask dimension and markers for Hippocampus region\n",
    "# '''\n",
    "# def markers(slices_mask):\n",
    "#     marks = []\n",
    "#     for i in range(len(slices_mask)):\n",
    "#         x_min = []\n",
    "#         y_min = []\n",
    "#         w_max = []\n",
    "#         h_max = []\n",
    "#         z_mn = 0\n",
    "#         indx_diff = []\n",
    "#         for j in range(len(slices_mask[i])):\n",
    "#             x_min.append(slices_mask[i][j][1][0])\n",
    "#             y_min.append(slices_mask[i][j][1][1])\n",
    "#             w_max.append(slices_mask[i][j][1][2])\n",
    "#             h_max.append(slices_mask[i][j][1][3])\n",
    "\n",
    "#             if slices_mask[i][j][1][0] != 0:\n",
    "#                 indx_diff.append(slices_mask[i][j][0])\n",
    "#         x_min = np.array(x_min)\n",
    "#         y_min = np.array(y_min)\n",
    "#         x_mn = np.min(x_min[np.nonzero(x_min)])\n",
    "#         y_mn = np.min(y_min[np.nonzero(y_min)])\n",
    "#         w_mx = np.array(w_max).max()\n",
    "#         h_mx = np.array(h_max).max()\n",
    "#         dim = [x_mn, y_mn, w_mx, h_mx]\n",
    "#         # print(dim)\n",
    "#         d_mx = indx_diff[-1] - indx_diff[0]\n",
    "#         # print(diff)\n",
    "#         z_mn = indx_diff[0]\n",
    "#         marks.append([x_mn, y_mn, z_mn, w_mx, h_mx , d_mx])\n",
    "#     return marks\n",
    "\n",
    "# sliced_markers_cn = markers(sliced_masks_cn)\n",
    "# sliced_markers_ad = markers(sliced_masks_ad)\n",
    "\n",
    "# ''' Concat labels 1 - AD,  0 -  CN\n",
    "# '''\n",
    "# labels_ad = np.ones((len(sliced_markers_ad),1))\n",
    "# labels_cn = np.zeros((len(sliced_markers_cn),1))\n",
    "\n",
    "# label_cn = np.hstack([sliced_markers_cn, labels_cn])\n",
    "# label_ad = np.hstack([sliced_markers_ad, labels_ad])\n",
    "\n",
    "\n",
    "# class data_cleaning_concat():\n",
    "#     def __init__(self, data1, data2, label1, label2):\n",
    "#         self.data1 = data1\n",
    "#         self.data2 = data2\n",
    "#         self.label1 = label1\n",
    "#         self.label2 = label2\n",
    "\n",
    "#     def _index_values(self, dataset):\n",
    "#         index = []\n",
    "#         for i in range(len(dataset)):\n",
    "#             if len(dataset[i]) < 145:\n",
    "#                 index.append(i)\n",
    "#         return index\n",
    "\n",
    "#     def _pop_list(self, dataset, label, index):\n",
    "#         for i in range(len(index)):\n",
    "#             dataset.pop(index[i])\n",
    "#             label.pop(index[i])\n",
    "#         return dataset, label\n",
    "\n",
    "#     def files_concat(self):\n",
    "#         concat_data = []\n",
    "#         concat_label = []\n",
    "\n",
    "#         data1_ , label1_ = self._pop_list(self.data1, self.label1, self._index_values(self.data1))\n",
    "#         data2_ , label2_ = self._pop_list(self.data2, self.label2, self._index_values(self.data2))\n",
    "\n",
    "#         for i in range(len(data1_)):\n",
    "#             concat_data.append(data1_[i])\n",
    "#             concat_label.append(label1_[i])\n",
    "\n",
    "#         for j in range(len(data2_)):\n",
    "#             concat_data.append(data2_[j])\n",
    "#             concat_label.append(label2_[j])\n",
    "\n",
    "#         return shuffle(concat_data, concat_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_clean = data_cleaning_concat(sliced_all_files_ad, sliced_all_files_cn,\n",
    "#                                                 list(label_ad) , list(label_cn))\n",
    "# total_data , total_label = data_clean.files_concat()\n",
    "\n",
    "# def cross_valid_splits(data, label, fold):\n",
    "#     fold_train = []\n",
    "#     fold_test = []\n",
    "#     fold_ytrain = []\n",
    "#     fold_ytest = []\n",
    "\n",
    "#     indxs1 = int(1*len(data)/5)\n",
    "#     indxs2 = int(2*len(data)/5)\n",
    "#     indxs3 = int(3*len(data)/5)\n",
    "#     indxs4 = int(4*len(data)/5)\n",
    "#     indxs5 = int(5*len(data)/5)\n",
    "\n",
    "#     rn_trn = []\n",
    "#     rn_tst = []\n",
    "#     if fold == 1:\n",
    "#         for i in range(0, indxs4):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "\n",
    "#         for j in range(indxs4, indxs5):\n",
    "#             fold_test.append(data[j])\n",
    "#             fold_ytest.append(label[j])\n",
    "\n",
    "#     elif fold == 2:\n",
    "#         for i in range(0, indxs3):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "#         for i in range(indxs4,indxs5):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "\n",
    "\n",
    "#         for j in range(indxs3, indxs4):\n",
    "#             fold_test.append(data[j])\n",
    "#             fold_ytest.append(label[j])\n",
    "\n",
    "#     elif fold == 3:\n",
    "#         for i in range(0, indxs2):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "#         for i in range(indxs3,indxs5):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "\n",
    "\n",
    "#         for j in range(indxs2, indxs3):\n",
    "#             fold_test.append(data[j])\n",
    "#             fold_ytest.append(label[j])\n",
    "\n",
    "\n",
    "#     elif fold == 4:\n",
    "#         for i in range(0, indxs1):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "#         for i in range(indxs2,indxs5):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "\n",
    "\n",
    "#         for j in range(indxs1, indxs2):\n",
    "#             fold_test.append(data[j])\n",
    "#             fold_ytest.append(label[j])\n",
    "\n",
    "\n",
    "#     elif fold == 5:\n",
    "#         for i in range(indxs1, indxs5):\n",
    "#             fold_train.append(data[i])\n",
    "#             fold_ytrain.append(label[i])\n",
    "\n",
    "#         for j in range(0, indxs1):\n",
    "#             fold_test.append(data[j])\n",
    "#             fold_ytest.append(label[j])\n",
    "\n",
    "\n",
    "#     return fold_train, fold_test, np.array(fold_ytrain), np.array(fold_ytest)\n",
    "\n",
    "\n",
    "# f_train1, f_test1, f_ytrain1, f_ytest1 = cross_valid_splits(total_data, total_label,1)\n",
    "\n",
    "# f_train2, f_test2, f_ytrain2, f_ytest2 = cross_valid_splits(total_data, total_label,2)\n",
    "\n",
    "# f_train3, f_test3, f_ytrain3, f_ytest3 = cross_valid_splits(total_data, total_label,3)\n",
    "\n",
    "# f_train4, f_test4, f_ytrain4, f_ytest4 = cross_valid_splits(total_data, total_label,4)\n",
    "\n",
    "# f_train5, f_test5, f_ytrain5, f_ytest5 = cross_valid_splits(total_data, total_label,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_brats_data...\n",
      "369 369 1845  - len(files_path), len(mask_paths), len(unique_id)\n",
      "['../archive_BraTS2022/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_082/BraTS20_Training_082_t1.nii', '../archive_BraTS2022/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_244/BraTS20_Training_244_t1.nii'] ['../archive_BraTS2022/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_082/BraTS20_Training_082_seg.nii', '../archive_BraTS2022/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_244/BraTS20_Training_244_seg.nii'] ['82', '82']  - files_path, mask_paths, unique_id\n",
      "load_brats_data done.\n",
      "compute_brats_masks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e689697043474c91222325f5aa91eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(369, 150, 2)  - np_all_masks.shape\n",
      "compute_brats_masks done.\n",
      "brats_markers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce9cee9ee7541e79010bc4625552bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[77, 51, 30, 108, 87, 82], [80, 56, 27, 85, 57, 54]]  - marks[:2]\n",
      "brats_markers done.\n",
      "load_brats_data...\n",
      "125 0 500  - len(files_path), len(mask_paths), len(unique_id)\n",
      "['../archive_BraTS2022/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_069/BraTS20_Validation_069_t1.nii', '../archive_BraTS2022/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/BraTS20_Validation_056/BraTS20_Validation_056_t1.nii'] [] ['69', '69']  - files_path, mask_paths, unique_id\n",
      "load_brats_data done.\n",
      "compute_brats_masks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b19fe057ed4dd8a0c3264006149257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)  - np_all_masks.shape\n",
      "compute_brats_masks done.\n",
      "brats_markers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4585b677d9a46df8c9a9c764e89a9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]  - marks[:2]\n",
      "brats_markers done.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import fnmatch\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from medpy.io import load\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "f_train1 = \"path to Training samples\"\n",
    "f_ytrain1 = \"path to Training labels\"\n",
    "f_test1 = \"path to Test samples\"\n",
    "f_ytest1 = \"path to Test labels\"\n",
    "\n",
    "DATA_PATH = '../archive_BraTS2022'\n",
    "TRAIN_PATH = f'{DATA_PATH}/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "TEST_PATH = f'{DATA_PATH}/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "DATA_TYPES = ['flair', 't1', 't1ce', 't2', 'seg']\n",
    "N_FOLDS = 5\n",
    "\n",
    "class data_set(Dataset):\n",
    "  def __init__(self, path=TRAIN_PATH, transform = None):\n",
    "    ### Variable definition to be used in Data loader\n",
    "    self.max_z = 150 # max z value to be used in the data\n",
    "    self.path = path\n",
    "    self.transform_train = transform\n",
    "    \n",
    "    self.file_paths, self.mask_paths, self.unique_id = self._load_brats_data(path)\n",
    "    # TODO: 10개만 load하는 것은 Test를 위한 것이므로, 추후에는 전체 데이터를 load해야함.\n",
    "    # self.file_paths, self.mask_paths, self.unique_id = self.file_paths[:10], self.mask_paths[:10], self.unique_id[:10]\n",
    "    self.all_masks = self._compute_brats_masks()\n",
    "    self.makers = np.array(self._brats_markers(self.all_masks))\n",
    "    \n",
    "    # TODO: 이 부분은 brats data를 위한 코드로 수정해야함.\n",
    "  def _load_brats_data(self, path):\n",
    "    print('load_brats_data...')\n",
    "    files_path = []\n",
    "    mask_paths = []\n",
    "    unique_id = []\n",
    "    for file_path in glob(f'{path}/*/*.nii'):\n",
    "        # t1 파일만 처리\n",
    "        if re.search(r\"_t1(?=\\.)\", file_path):\n",
    "            files_path.append(file_path)\n",
    "        # seg 파일만 처리\n",
    "        if \"seg\" in file_path:  # segmentation mask file\n",
    "            mask_paths.append(file_path)\n",
    "        unique_id.append(str(int(file_path.split('_')[-2])))\n",
    "    \n",
    "    print(len(files_path), len(mask_paths), len(unique_id), \" - len(files_path), len(mask_paths), len(unique_id)\")\n",
    "    print(files_path[:2], mask_paths[:2], unique_id[:2], \" - files_path, mask_paths, unique_id\")\n",
    "    print(\"load_brats_data done.\")\n",
    "    return sorted(files_path), sorted(mask_paths), sorted(list(set(unique_id)), key=lambda x: int(x))\n",
    "\n",
    "  def _compute_brats_masks(self):\n",
    "      print('compute_brats_masks...')\n",
    "      all_masks = []\n",
    "      for mask_path in tqdm(self.mask_paths):\n",
    "        mask = nib.load(mask_path).get_fdata()  # 마스크 데이터 로드\n",
    "        dummy_var = []\n",
    "        # visualization.ipynb를 참고\n",
    "        for j in range(self.max_z):\n",
    "          msk = mask[:, :, j] / mask.max() * 255  # 슬라이스 정규화 및 스케일링(0 ~ 1)\n",
    "          # msk = cv2.resize(msk, (256, 256))  # 이미지 크기 조정\n",
    "          _, bw_img = cv2.threshold(msk.astype(np.uint8), 10, 255, cv2.THRESH_OTSU)  # 이진화\n",
    "          # contours = [x_min, y_min, w, h]를 의미함.\n",
    "          contours, _ = cv2.findContours(bw_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)  # 컨투어 찾기\n",
    "          \n",
    "          # 가장 큰 컨투어 찾기\n",
    "          max_area = 0\n",
    "          max_rect = (0, 0, 0, 0)\n",
    "          for contour in contours:\n",
    "              rect = cv2.boundingRect(contour)\n",
    "              area = rect[2] * rect[3]  # rectangle의 넓이 (width * height)\n",
    "              if area > max_area:\n",
    "                  max_area = area\n",
    "                  max_rect = rect\n",
    "          \n",
    "          # 가장 큰 컨투어의 사각형 정보 저장\n",
    "          if max_area > 0:\n",
    "              dummy_var.append([j, max_rect])\n",
    "          else:\n",
    "              dummy_rect = [0, 0, 0, 0]  # 컨투어가 없을 경우\n",
    "              dummy_var.append([j, dummy_rect])\n",
    "        all_masks.append(dummy_var)\n",
    "      \n",
    "      # print(all_masks[:10], \"- all_masks[:10]\")\n",
    "      np_all_masks = np.array(all_masks, dtype=object)\n",
    "      print(np_all_masks.shape, \" - np_all_masks.shape\")\n",
    "      print(\"compute_brats_masks done.\")  \n",
    "      return all_masks\n",
    "\n",
    "\n",
    "  def _brats_markers(self, slices_mask):\n",
    "      print('brats_markers...')\n",
    "      marks = []\n",
    "      for i in tqdm(range(len(slices_mask))):\n",
    "          x_min = []\n",
    "          y_min = []\n",
    "          w_max = []\n",
    "          h_max = []\n",
    "          indx_diff = []\n",
    "          for j in range(len(slices_mask[i])):\n",
    "              x_min.append(slices_mask[i][j][1][0])\n",
    "              y_min.append(slices_mask[i][j][1][1])\n",
    "              w_max.append(slices_mask[i][j][1][2])\n",
    "              h_max.append(slices_mask[i][j][1][3])\n",
    "\n",
    "              if slices_mask[i][j][1][0] != 0:\n",
    "                  indx_diff.append(slices_mask[i][j][0])\n",
    "          x_min = np.array(x_min)\n",
    "          y_min = np.array(y_min)\n",
    "          x_mn = np.min(x_min[np.nonzero(x_min)])\n",
    "          y_mn = np.min(y_min[np.nonzero(y_min)])\n",
    "          w_mx = np.array(w_max).max()\n",
    "          h_mx = np.array(h_max).max()\n",
    "          dim = [x_mn, y_mn, w_mx, h_mx]\n",
    "          d_mx = indx_diff[-1] - indx_diff[0]\n",
    "          z_mn = indx_diff[0]\n",
    "          marks.append([x_mn, y_mn, z_mn, w_mx, h_mx, d_mx])\n",
    "      print(marks[:2], \" - marks[:2]\")\n",
    "      print(\"brats_markers done.\")\n",
    "      return marks\n",
    "\n",
    "  def __len__(self):\n",
    "    ### size of file path\n",
    "    return len(self.file_paths)\n",
    "\n",
    "  def __getitem__(self, indx):\n",
    "    '''\n",
    "      Read the image given indx from len function\n",
    "    '''\n",
    "    # image_input = np.ones((145,256,256))\n",
    "    # image_input = np.ones(self.shape) # (240, 240, 155 - 5)\n",
    "    #for j in range(indx):\n",
    "    # 아래의 for loop는 145개의 slice를 읽어오는 코드임.\n",
    "    # 근데 이미 우리는 3D image가 있어서, medpy.io.load로 읽어오면 됨.\n",
    "    # cv2로 필요없음, 다른 부분의 code를 수정해서 (240, 240, 155)\n",
    "    # for i in range(image_input[-1]):\n",
    "    #     try:\n",
    "    #         img, _ = load(self.file_path[indx][i])\n",
    "    #         # img = cv2.resize(img.squeeze(), (256, 256))\n",
    "    #         \n",
    "    #         # img = self.transform_train(img.squeeze().astype('float'))\n",
    "    #         image_input[:, :, i] = img\n",
    "    #     except:\n",
    "    #         print(\"Check Index: \", indx)\n",
    "    \n",
    "    # box_gd = np.array([np.array(self.label)[indx][0], np.array(self.label)[indx][1], np.array(self.label)[indx][2],\n",
    "    #           np.array(self.label)[indx][3], np.array(self.label)[indx][4], np.array(self.label)[indx][5]])\n",
    "\n",
    "\n",
    "    # TODO : Transformation을 추가해야함.\n",
    "    t1_image = nib.load(self.file_paths[indx]).get_fdata()\n",
    "    box_gd = self.makers[indx]\n",
    "\n",
    "    # 우리는 label이 필요없으므로 삭제.\n",
    "    # label_ = np.array(self.label)[indx][6]\n",
    "    return [t1_image, box_gd]\n",
    "\n",
    "\n",
    "\n",
    "# train_data = data_set(file_path = f_train1, label = f_ytrain1, transform_train = transforms.Compose([\n",
    "#                                                                                    transforms.ToTensor(),\n",
    "#                                                                                 # TODO: t1 image에 Normalize하는게 맞는지 생각해야 함.\n",
    "#                                                                                 #    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                                                                    ]))\n",
    "\n",
    "# unseen_data = data_set(file_path = f_test1, label = f_ytest1, transform_train = transforms.Compose([\n",
    "#                                                                                    transforms.ToTensor(),\n",
    "#                                                                                 #    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#                                                                                    ]))\n",
    "\n",
    "train_data = data_set(path = TRAIN_PATH)\n",
    "unseen_data = data_set(path = TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(train_data, batch_size = 64,\n",
    "                        shuffle= True, num_workers= 0)\n",
    "dataloader_unseen = torch.utils.data.DataLoader(unseen_data, batch_size = 1,\n",
    "                        shuffle= True, num_workers= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  - epoch\n",
      "0  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   0%|          | 0/100 [00:00<?, ?it/s]/var/folders/vd/92v8_14d7n7gsy5vw_3gc2wm0000gn/T/ipykernel_20362/933098599.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dqn_model.store_transition(prev_patch, np.asarray(int(action)), reward, nxt_patch)\n",
      "Episodes:  27%|██▋       | 27/100 [00:04<00:12,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [111, 42, 18, 50, 60, 50] , ground truth [62 59 35 93 80 90], episodes 27, batch 0\n",
      "1  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [81, 61, 26, 50, 60, 50] , ground truth [99 73 31 94 38 53], episodes 99, batch 1\n",
      "2  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:17<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [21, 86, 35, 50, 60, 50] , ground truth [47 67 55 69 74 62], episodes 99, batch 2\n",
      "3  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:17<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [81, 21, 15, 50, 60, 50] , ground truth [ 52 104  31  95  70  93], episodes 99, batch 3\n",
      "4  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:18<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [76, 95, 2, 50, 60, 50] , ground truth [120  64  48  65  59  79], episodes 99, batch 4\n",
      "5  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:  26%|██▌       | 26/100 [00:04<00:11,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [93, 46, 22, 50, 60, 50] , ground truth [104  56  54  83  68  70], episodes 26, batch 5\n",
      "6  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:17<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Pred [64, 40, 35, 50, 60, 50] , ground truth [ 98 140  32  78  44  43], episodes 99, batch 6\n",
      "7  - batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:  79%|███████▉  | 79/100 [00:14<00:03,  6.08it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d,MaxUnpool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from skimage import util\n",
    "from torchvision import transforms, models\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def training_phase(dqn_model, dataloader):\n",
    "    training_acc = []\n",
    "    training_loss = []\n",
    "    for epoch in range(100):\n",
    "        print(epoch, \" - epoch\")\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        total = 0.0\n",
    "        for data in dataloader:\n",
    "            # images, box_gds, labels = data\n",
    "            images, box_gds = data\n",
    "            batches, height, width, depth = images.shape\n",
    "            dqn_mask = []\n",
    "            loss_batch = torch.tensor(np.ones(batches))\n",
    "            reward_batch = torch.tensor(np.ones(batches))\n",
    "            dqn_mask = torch.tensor(np.ones((batches,1,50,height,width)))\n",
    "            for batch in range(batches):\n",
    "                print(batch, \" - batch\")\n",
    "                # TODO: x, y, z가 80~120, 60~150, 50~95로 시작하는데, 이거는 따로 우리의 데이터에 맞게 수정해야함.\n",
    "                x_start = np.random.randint(80,120)\n",
    "                y_start = np.random.randint(60,200-50)\n",
    "                z_start = np.random.randint(50,145-50)\n",
    "                w, h, d = 50, 60, 50\n",
    "                start_state = [x_start, y_start, z_start, w, h, d]\n",
    "                game = \"continue\"\n",
    "                env = images[batch]\n",
    "                # 3D Labeling으로 x, y, z, w, h ,d가 필요함.\n",
    "           \n",
    "                x_gd, y_gd, z_gd = box_gds[batch][0], box_gds[batch][1], box_gds[batch][2]\n",
    "                w_gd, h_gd, d_gd = box_gds[batch][3], box_gds[batch][4], box_gds[batch][5]\n",
    "                state_gd = [x_gd, y_gd, z_gd, w_gd, h_gd, d_gd]\n",
    "                start_eps = 1\n",
    "                end_eps = 0.004\n",
    "                start_step = 20\n",
    "                end_step = 5\n",
    "                # print(start_state, state_gd, \" - start_state, state_gd\")\n",
    "                prev_state = start_state.copy()\n",
    "                loss = 0\n",
    "                rewards = 0\n",
    "                for episodes in tqdm(range(100), desc = \"Episodes\"):\n",
    "                    eps = max(0,(start_eps/int(1 + episodes)   - end_eps))\n",
    "                    step = max(end_step, start_step/int(1 + episodes) )\n",
    "\n",
    "                    # TODO: crop_reshape 함수가 하는 역할을 이해하고 우리의 데이터에 맞게 수정해야함.\n",
    "                    # crop_reshpe는 3D로 바꿔주는 함수, crop_reshape(env, x, y, z, w, h, d) -> env[x:x+w, y:y+h, z:z+d]\n",
    "                    # 즉, 원래 목적은 shape이 맞지 않은 3D image를 interpolation을 통해서 3D로 확장했음.\n",
    "                    # 우리의 데이터는 그럴 필요가 없기 때문에, 바로 넘겨줘도 될 듯 함.\n",
    "                    # 다만 prev_state가 정확히 어떤 의미인지 잘 모르겟음\n",
    "                    # prev_state : [x, y, z, w, h, d]\n",
    "                    prev_patch = crop_reshape(env, prev_state[0],prev_state[1],prev_state[2],prev_state[3],prev_state[4],prev_state[5])\n",
    "                    action = dqn_model.select_action(prev_patch, game, eps)\n",
    "                    # print(action, \"- action\")\n",
    "                    if action != 6:\n",
    "                        nxt_state = dqn_model.next_state(prev_state, action, step)\n",
    "                        nxt_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "                        # print(nxt_state, \" - nxt_state\")\n",
    "                        # print(np.shape(nxt_patch), \" - nxt_patch.shape\")\n",
    "                        game, reward = dqn_model.compute_reward(nxt_state, prev_state, state_gd, 10)\n",
    "                    else:\n",
    "                        game = \"END\"\n",
    "                        nxt_state = prev_state.copy()\n",
    "                        nxt_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "                        # print(nxt_state, \" - nxt_state\")\n",
    "                        # print(np.shape(nxt_patch), \" - nxt_patch.shape\")\n",
    "                        _, reward = dqn_model.compute_reward(nxt_state, prev_state, state_gd, 10)\n",
    "                    #print(np.asarray(action))\n",
    "                    dqn_model.store_transition(prev_patch, np.asarray(int(action)), reward, nxt_patch)\n",
    "\n",
    "                    if dqn_model.memory.memory_counter >= 1000:\n",
    "                        loss = dqn_model.optimize(1500)\n",
    "\n",
    "\n",
    "                    prev_state = nxt_state.copy()\n",
    "                    prev_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "                    rewards += reward\n",
    "                    #print(\"Episodes : \", episodes)\n",
    "                    if game == \"END\" or episodes == 2000:\n",
    "                        dqn_mask[batch] = nxt_patch.view(1, nxt_patch.shape[-3], nxt_patch.shape[-2],nxt_patch.shape[-1])\n",
    "                        loss_batch[batch] = (loss/int(1 + episodes))\n",
    "                        reward_batch[batch] = (rewards/int(1 + episodes))\n",
    "                        break\n",
    "                print(\"State Pred {:} , ground truth {:}, episodes {:}, batch {:}\".format(nxt_state , np.array(state_gd), episodes, batch))\n",
    "            try:\n",
    "                loss_per_batch = loss_batch.numpy().detach()\n",
    "            except:\n",
    "                loss_per_batch = loss_batch\n",
    "\n",
    "            # print(loss_per_batch.shape)\n",
    "            print(\"Loss_Rl {:.4f} \\tReward_RL {:.4f}\".format (loss_per_batch.mean(), reward_batch.mean()))\n",
    "        # print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.6f}'.format(epoch, train_loss, train_accuracy))\n",
    "    return training_acc, training_loss\n",
    "\n",
    "t_phase1, l_phase1 = training_phase(my_dqn,  dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.autograd import Variable\n",
    "# from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d,MaxUnpool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "# from torch.optim import Adam, SGD\n",
    "# from skimage import util\n",
    "# from torchvision import transforms, models\n",
    "# from torch import optim\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import cv2\n",
    "# import torchvision\n",
    "# import math\n",
    "# from sklearn.utils import shuffle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def testing_phase(dqn_model,classifier_model, dataloader, criterion):\n",
    "#     testing_acc = []\n",
    "#     testing_loss = []\n",
    "#     pred_test = []\n",
    "#     pred_true = []\n",
    "#     final_box = []\n",
    "#     test_loss = 0.0\n",
    "#     test_acc = 0.0\n",
    "#     total = 0.0\n",
    "#     for data in dataloader:\n",
    "#         images, box_gds, labels = data\n",
    "#         batches, depth, height, width = images.shape\n",
    "#         loss_batch = torch.tensor(np.ones(batches))\n",
    "#         reward_batch = torch.tensor(np.ones(batches))\n",
    "#         dqn_mask = torch.tensor(np.ones((batches,1,50,height,width)))\n",
    "#         for batch in range(batches):\n",
    "#             x_start = np.random.randint(80,120)\n",
    "#             y_start = np.random.randint(60,200-50)\n",
    "#             z_start = np.random.randint(50,145-50)\n",
    "#             w, h, d = 50, 60, 50\n",
    "#             start_state = [x_start, y_start, z_start, w, h, d]\n",
    "#             game = \"continue\"\n",
    "#             env = images[batch]\n",
    "#             x_gd, y_gd, z_gd = box_gds[batch][0], box_gds[batch][1], box_gds[batch][2]\n",
    "#             w_gd, h_gd, d_gd = box_gds[batch][3], box_gds[batch][4], box_gds[batch][5]\n",
    "#             state_gd = [x_gd, y_gd, z_gd, w_gd, h_gd, d_gd]\n",
    "#             start_eps = 1\n",
    "#             end_eps = 0.004\n",
    "#             start_step = 20\n",
    "#             end_step = 5\n",
    "#             prev_state = start_state.copy()\n",
    "#             loss = 0\n",
    "#             rewards = 0\n",
    "#             for episodes in range(500):\n",
    "#                 eps = max(0,(start_eps/int(1 + episodes)   - end_eps))\n",
    "#                 step = max(end_step, start_step/int(1 + episodes) )\n",
    "\n",
    "#                 prev_patch = crop_reshape(env, prev_state[0],prev_state[1],prev_state[2],prev_state[3],prev_state[4],prev_state[5])\n",
    "#                 action = dqn_model.select_action_test(prev_patch)\n",
    "#                 if action != 6:\n",
    "#                     nxt_state = dqn_model.next_state(prev_state, action, step)\n",
    "#                     # print(nxt_state)\n",
    "#                     nxt_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "#                     game, reward = dqn_model.compute_reward(nxt_state, prev_state, state_gd, 10)\n",
    "#                 else:\n",
    "#                     game = \"END\"\n",
    "#                     nxt_state = prev_state.copy()\n",
    "#                     nxt_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "#                     _, reward = dqn_model.compute_reward(nxt_state,prev_state, state_gd, 10)\n",
    "#                     #print(np.asarray(action))\n",
    "#                 prev_state = nxt_state.copy()\n",
    "#                 prev_patch = crop_reshape(env, nxt_state[0],nxt_state[1],nxt_state[2],nxt_state[3],nxt_state[4],nxt_state[5])\n",
    "#                 rewards += reward\n",
    "#                 #print(\"Episodes : \", episodes)\n",
    "#                 if game == \"END\" or episodes == 500:\n",
    "#                     dqn_mask[batch] = nxt_patch.view(1, nxt_patch.shape[-3], nxt_patch.shape[-2],nxt_patch.shape[-1])\n",
    "#                     loss_batch[batch] = (loss/int(1 + episodes))\n",
    "#                     reward_batch[batch] = (rewards/int(1 + episodes))\n",
    "#                     break\n",
    "#             print(\"State Pred {:} , ground truth {:}, episodes {:}, batch {:}\".format(nxt_state , np.array(state_gd), episodes, batch))\n",
    "#         try:\n",
    "#             loss_per_batch = loss_batch.numpy().detach()\n",
    "#         except:\n",
    "#             loss_per_batch = loss_batch\n",
    "\n",
    "#         # print(loss_per_batch.shape)\n",
    "#         print(\"Loss_Rl {:.4f} \\tReward_RL {:.4f}\".format (loss_per_batch.mean(), reward_batch.mean()))\n",
    "#         out_pred = classifier_model(dqn_mask.float(), images.view(batches, 1, depth, height, width).float())\n",
    "#         loss = criterion(out_pred, labels.type(torch.LongTensor))#.view(-1,1) float()\n",
    "#         test_loss += loss.item()\n",
    "#         total += labels.size(0)\n",
    "#         test_acc +=  binary_acc(out_pred, labels)\n",
    "#         pred_test.append(int(out_pred.detach()))\n",
    "#         pred_true.append(int(labels))\n",
    "#         final_box.append([nxt_state, np.array(state_gd)])\n",
    "#     test_loss = test_loss/len(dataloader)\n",
    "#     testing_loss.append(test_loss)\n",
    "#     test_acc = test_acc/total #100 *\n",
    "#     testing_acc.append(test_acc)\n",
    "#     print('Test Loss: {:.6f} \\tTest Accuracy: {:.6f}'.format( test_loss, test_acc))\n",
    "#     return pred_test, final_box, test_loss, test_acc, pred_true\n",
    "\n",
    "\n",
    "# a,b,c,d,e = testing_phase(my_dqn, class_model,  dataloader_unseen, criterion )\n",
    "\n",
    "\n",
    "# def bar_plot(output_pred, output_true, unseen_loss, unseen_acc):\n",
    "#   plt.figure(2)\n",
    "#   cm = confusion_matrix(np.array(output_pred).squeeze(), np.array(output_true).squeeze())\n",
    "#   recall = 100*cm[0,0]/(cm[0,0] + cm[1,0])\n",
    "#   precision = 100*cm[0,0]/(cm[0,0] + cm[0,1])\n",
    "#   params = ['Loss', 'Accuracy', 'TN', 'FP', 'FN', 'TP', 'Precision', 'Recall']\n",
    "#   results = [unseen_loss, unseen_acc,cm[1,1],cm[0,1],cm[1,0],cm[0,0], precision, recall]\n",
    "#   plt.bar(params, results)\n",
    "#   for index,data in enumerate(results):\n",
    "#     plt.text(x = index -0.2 , y = data  , s = f\"{int(data)}\" , fontdict=dict(fontsize=10))\n",
    "#   plt.tight_layout()\n",
    "#   plt.show()\n",
    "\n",
    "# bar_plot(a,e,c,100*d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
